{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports for Deep Learning\nfrom keras.layers import Conv2D, Dense, Dropout, Flatten\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# ensure consistency across runs\nfrom numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)\n\n# Imports to view data\nimport cv2\nfrom glob import glob\nfrom matplotlib import pyplot as plt\nfrom numpy import floor\nimport random\n\ndef plot_three_samples(letter):\n    print(\"Samples images for letter \" + letter)\n    base_path = '../input/asl_alphabet_train/asl_alphabet_train/'\n    img_path = base_path + letter + '/**'\n    path_contents = glob(img_path)\n    \n    plt.figure(figsize=(16,16))\n    imgs = random.sample(path_contents, 3)\n    plt.subplot(131)\n    plt.imshow(cv2.imread(imgs[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(imgs[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(imgs[2]))\n    return\n\nplot_three_samples('A')","metadata":{"_uuid":"a334a3525c03f23a55668f55db390abca81eab20","_kg_hide-output":true,"_cell_guid":"a5f1f752-8365-40a8-a849-8ab2ab34db63","execution":{"iopub.status.busy":"2022-05-12T00:40:57.028031Z","iopub.execute_input":"2022-05-12T00:40:57.028334Z","iopub.status.idle":"2022-05-12T00:41:00.707470Z","shell.execute_reply.started":"2022-05-12T00:40:57.028274Z","shell.execute_reply":"2022-05-12T00:41:00.705437Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"plot_three_samples('B')","metadata":{"_kg_hide-input":true,"_cell_guid":"55c4a665-6a03-419d-9868-68a4c9cbdf71","_uuid":"b3ef9cdbfebfc6cde56519c9163d1f8aa604627e","execution":{"iopub.status.busy":"2022-05-12T00:41:00.708595Z","iopub.execute_input":"2022-05-12T00:41:00.708861Z","iopub.status.idle":"2022-05-12T00:41:01.478536Z","shell.execute_reply.started":"2022-05-12T00:41:00.708819Z","shell.execute_reply":"2022-05-12T00:41:01.477919Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing Set-Up","metadata":{"_uuid":"3c0043a20407434fa301f51565bb184b4b54cde3","_cell_guid":"a6106ce0-3618-43c6-bf05-a8e64fb4cbdd"}},{"cell_type":"code","source":"data_dir = \"../input/asl_alphabet_train/asl_alphabet_train\"\ntarget_size = (64, 64)\ntarget_dims = (64, 64, 3) # add channel for RGB\nn_classes = 29\nval_frac = 0.1\nbatch_size = 64\n\ndata_augmentor = ImageDataGenerator(samplewise_center=True, \n                                    samplewise_std_normalization=True, \n                                    validation_split=val_frac)\n\ntrain_generator = data_augmentor.flow_from_directory(data_dir, target_size=target_size, batch_size=batch_size, shuffle=True, subset=\"training\")\nval_generator = data_augmentor.flow_from_directory(data_dir, target_size=target_size, batch_size=batch_size, subset=\"validation\")","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-05-12T00:41:01.479712Z","iopub.execute_input":"2022-05-12T00:41:01.480194Z","iopub.status.idle":"2022-05-12T00:42:27.216677Z","shell.execute_reply.started":"2022-05-12T00:41:01.480141Z","shell.execute_reply":"2022-05-12T00:42:27.216042Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Model Specification","metadata":{"_uuid":"1fab4c91abce8e388652ef9c4c3d3ed6a3489a7c","_cell_guid":"15dc1ae0-bc4a-4988-9c8c-01a9f8b5524c"}},{"cell_type":"code","source":"my_model = Sequential()\nmy_model.add(Conv2D(64, kernel_size=4, strides=1, activation='relu', input_shape=target_dims))\nmy_model.add(Conv2D(64, kernel_size=4, strides=2, activation='relu'))\nmy_model.add(Dropout(0.5))\nmy_model.add(Conv2D(128, kernel_size=4, strides=1, activation='relu'))\nmy_model.add(Conv2D(128, kernel_size=4, strides=2, activation='relu'))\nmy_model.add(Dropout(0.5))\nmy_model.add(Conv2D(256, kernel_size=4, strides=1, activation='relu'))\nmy_model.add(Conv2D(256, kernel_size=4, strides=2, activation='relu'))\nmy_model.add(Flatten())\nmy_model.add(Dropout(0.5))\nmy_model.add(Dense(512, activation='relu'))\nmy_model.add(Dense(n_classes, activation='softmax'))\n\nmy_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])","metadata":{"_uuid":"dc201ab00d5790a06511a86563920f90a3f8f2d5","_cell_guid":"435128bc-b3cb-410e-85a7-3ef43748b3f8","execution":{"iopub.status.busy":"2022-05-12T00:42:27.217981Z","iopub.execute_input":"2022-05-12T00:42:27.218281Z","iopub.status.idle":"2022-05-12T00:42:27.406121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Fitting","metadata":{"_uuid":"254cf9f24aecdc1150f9dbb5e47e53d15362320c","_cell_guid":"d8be6e96-7e26-4cf9-b8a8-f4a7074be4bf"}},{"cell_type":"code","source":"my_model.fit_generator(train_generator, epochs=5, validation_data=val_generator)","metadata":{"_uuid":"655cfaa648459d757a204d81269395591771c5fb","_cell_guid":"545599f9-c500-40b9-9853-22105572d82e","execution":{"iopub.status.busy":"2022-05-12T00:42:27.407268Z","iopub.execute_input":"2022-05-12T00:42:27.407587Z","iopub.status.idle":"2022-05-12T01:01:03.117380Z"},"trusted":true},"execution_count":null,"outputs":[]}]}